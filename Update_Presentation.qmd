---
title: "Update: Total PCA v. OLS"
author: "Art Tay"
format:
    beamer:
        theme: Berkeley
        include-in-header:
            text: |
                \captionsetup{labelfont = bf}
---

```{r setup, include=FALSE}
##Setup code
knitr::opts_chunk$set(warning = FALSE, message = FALSE)

# Libraries
library(tidyverse)
library(kableExtra)
library(tidymodels)
library(factoextra)
```


# Table of Contents
\tableofcontents

#
\section{Theory}

# Background
## OLS Refresher
Let $X$ be a $n \times p$ centered design matrix, and $Y$ be the $n \times 1$
response vector. Then the projection matrix $H$ that projects $Y$ to $\hat Y$,
the closest value to $Y$ in the $col(X)$ is given by
$$
\hat Y = HY = X(X^TX)^{-1}X^TY
$$

## PCA Refresher
Since $X^TX$ is symmetric, it can be decomposed into $PDP^T$, where $P$ is the
$p \times p$ orthonormal matrix of the eigenvectors of $X^TX$.

- This means that $P$ here is equivalent to the "PCA rotation matrix".

# Intuition

- We will define a "Total PCA Transformation" to be $Z = XP$.
\vspace{1cm}

- $Z$ is a $n \times p$ matrix that is a linear transformation of $X$. Therefore $col(Z) = col(X)$.
\vspace{1cm}

- Since the predictor space has just been rotated, the span hasn't changed and $\hat Y$ should still be the closed point to $Y$ in the new predictor space $Z$.

# Proof
- Let $H'$ be the projection matrix that projects $Y$ onto $Z$.
\begin{align}
    H' &= Z (Z^TZ)^{-1}Z^T \\
       &= XP ((XP)^T XP)^{-1}(XP)^T \\
       &= XP (P^T X^TX P)^{-1}P^TX^T \\
       &= X PP^T(X^TX)^{-1} PP^T X^T \\
       &= X (X^TX)^{-1} X^T
\end{align}

- Thus $\hat Y_{PCA} = \hat Y_{OLS} \Rightarrow$ the Training and Testing errors
must be identical between the two methods.

#
\section{Code}

# Problem
- Old Code:
```{r, eval = F, echo = T}
pca_recipe <- recipe(Y ~ .,
    data = dummy_dataset) %>%
    step_pca(all_numeric_predictors(),
        threshold = 1)
```

- New Code:
```{r, eval = F, echo = T}
pca_recipe <- recipe(Y ~ .,
    data = dummy_dataset) %>%
    step_pca(all_numeric_predictors(),
        num_comp = 30)
```

#
\section{Potential Problem}

# What could this mean?
- There are some samples where certain principal components explain 0 additional
variance.
\vspace{.5cm}

- Those principal components have a corresponding eigenvalue of 0.
\vspace{.5cm}

- $det(X^TX) = 0 \iff (X^TX)^{-1}$ does not exist $\iff$ Perfect Collinearity.
\vspace{.5cm}

- The model will still "fit", but it will be uninterpretable.

# Toy Example
- Suppose a true data generating model:
$$
Y = X + Z + \epsilon
$$
where $X, Z, \epsilon$ are all $\sim N(0, 1)$.
\vspace{1.5cm}
- But suppose a third predictor, $W = X + \frac 1 2 Z$ is included.

```{r}
x <- rnorm(10)
z <- rnorm(10)
y <- 2*x + 3*z + rnorm(10)

toy_df <- as.data.frame(cbind(y, x, z, x + 0.5*z + 1))

toy_ols <- lm(y ~., data = toy_df)
#summary(toy_ols)

pca_toy <- prcomp(toy_df[, -1], center = T, scale = T)
#fviz_eig(pca_toy)

pcr_df <- as.data.frame(cbind(y, pca_toy$x))

toy_pcr <- lm(y ~., data = pcr_df)
#summary(toy_pcr)

toy_pcr_2 <- lm(y ~ PC1 + PC2, data = pcr_df)
#summary(toy_pcr_2)
```

# Toy Example
:::: {.columns}

::: {.column width="50%"}
```{r}
toy_var <- pca_toy$sdev^2
pve <- toy_var/sum(toy_var)
pve_df <- as.data.frame(cbind(1:3, cumsum(pve)))
colnames(pve_df) <- c("Components", "% Variance Explained")

pve_df %>% ggplot(aes(
    x = Components, y = `% Variance Explained`
    )
) + geom_line() + geom_point() + theme_bw() +
    theme(text = element_text(size = 24))
```
:::

::: {.column width="50%"}
```{r}
models <- c("OLS", "PC1-2", "PC1-3")
MSE <- c(0.7579, 0.7579, 0.7165)
RMSE <- round(sqrt(MSE), 4)
df <- cbind(models, MSE, RMSE)
df %>% kbl(format = 'latex', booktabs = T,
    longtable = T, linesep = "")
```
:::

::::

- Maintaining all components under perfect correlation can "trick" the
algorithm into using useless information, because it will think its
an uncorrelated predictor.

#
\section{Actual Problem}

# Unexpected Behavior
```{r}
#| fig-cap: "Number of Components Retained by Total PCA Models Under the Old Code"
#| out-width: 80%
load("pca_param_count.rds")
pca_param_count %>% as.data.frame() %>%
    ggplot(aes(x = pca_param_count)) +
    geom_histogram() + theme_bw() +
    xlab("Parameters Used") + ylab("Frequency") +
    theme(text = element_text(size = 24))
```

#
\section*{Appendix}

# Low Correlation Case
```{r}
load('Figures/table_low.rds')
caption <- "Modeling results under low correlation between parameters"
table_low %>% kbl(format = 'latex', booktabs = T,
    longtable = T, linesep = "", caption = caption,
    align = "c", escape = F) %>%
    kable_styling(font_size = 10) %>%
    footnote(number = c("$99\\\\%$ mean t confidence intervals.", "Parameters means non-zero for LASSO type models and significant at $\\\\alpha$ = 0.05 otherwise."), threeparttable = T, escape = F)
```

# Moderate Correlation Case
```{r}
load('Figures/table_med.rds')
caption <- "Modeling results under moderate correlation between parameters"
table_med %>% kbl(format = 'latex', booktabs = T,
    longtable = T, linesep = "", caption = caption,
    align = "c", escape = F) %>%
    kable_styling(font_size = 10) %>%
    footnote(number = c("$99\\\\%$ mean t confidence intervals.", "Parameters means non-zero for LASSO type models and significant at $\\\\alpha$ = 0.05 otherwise."), threeparttable = T, escape = F)
```

# High Correlation Case
```{r}
load('Figures/table_high.rds')
caption <- "Modeling results under high correlation between parameters"
table_high %>% kbl(format = 'latex', booktabs = T,
    longtable = T, linesep = "", caption = caption,
    align = "c", escape = F) %>%
    kable_styling(font_size = 10) %>%
    footnote(number = c("$99\\\\%$ mean t confidence intervals.", "Parameters means non-zero for LASSO type models and significant at $\\\\alpha$ = 0.05 otherwise."), threeparttable = T, escape = F)
```

```{r, eval = F}
load("train_high.rds")
sample_1 <- as.data.frame(train_high[, 1])

#ols_fit_1 <- lm(Y ~., data = sample_1)
#summary(ols_fit_1)

pca_1 <- princomp(sample_1[, -1], cor = T)

print(loadings(pca_1))

fviz_eig(pca_1)
```

```{r, eval = F}
pca_recipe <- recipe(Y ~ ., data = sample_1) %>%
    step_pca(all_numeric_predictors(), threshold = 1)

summary(pca_recipe)

pca_tidy <- pca_recipe %>%
    prep() %>% bake(new_data = NULL)
```

```{r, eval = F}
pca.var <- pca_1$sdev^2
pve <- pca.var/sum(pca.var)
pve_df <- as.data.frame(cbind(1:30, cumsum(pve)))
colnames(pve_df) <- c("Components", "% Variance Explained")

pve_df %>% ggplot(aes(
    x = Components, y = `% Variance Explained`
)) + geom_line() + theme_bw()
```